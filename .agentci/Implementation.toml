[eval]
description = "Multi-dimensional quality assessment for Aura responses"
type = "llm"
targets.agents = ["technical-support", "aura-assistant"]
targets.tools = []

[eval.llm]
model = "gpt-4"
prompt = """
Evaluate this Aura response across multiple dimensions:

1. Technical Accuracy (1-10): Is the technical information correct?
2. Clarity (1-10): Is the explanation clear and easy to follow?
3. Completeness (1-10): Does it address all aspects of the question?
4. Actionability (1-10): Are the steps concrete and actionable?
5. Professional Tone (1-10): Is the tone appropriate and helpful?

For each dimension, also map the score into a label:
1–3 = "Poor"
4–6 = "Fair"
7–8 = "Good"
9–10 = "Excellent"

Finally, provide:
- An overall_score (1–10)
- An overall_label (Poor, Fair, Good, Excellent)
- Feedback: a short explanation summarizing strengths & weaknesses
"""

output_schema = """
{
  "type": "object",
  "required": [
    "technical_accuracy", "technical_accuracy_label",
    "clarity", "clarity_label",
    "completeness", "completeness_label",
    "actionability", "actionability_label",
    "professional_tone", "professional_tone_label",
    "overall_score", "overall_label", "feedback"
  ],
  "properties": {
    "technical_accuracy": {"type": "number", "minimum": 1, "maximum": 10},
    "technical_accuracy_label": {"type": "string"},
    "clarity": {"type": "number", "minimum": 1, "maximum": 10},
    "clarity_label": {"type": "string"},
    "completeness": {"type": "number", "minimum": 1, "maximum": 10},
    "completeness_label": {"type": "string"},
    "actionability": {"type": "number", "minimum": 1, "maximum": 10},
    "actionability_label": {"type": "string"},
    "professional_tone": {"type": "number", "minimum": 1, "maximum": 10},
    "professional_tone_label": {"type": "string"},
    "overall_score": {"type": "number", "minimum": 1, "maximum": 10},
    "overall_label": {"type": "string"},
    "feedback": {"type": "string"}
  }
}
"""

[[eval.cases]]
prompt = "My Aura dashboard isn’t syncing with my cloud files. What should I do?"
score = { min = 7 }
criteria = ["technical_accuracy", "actionability", "clarity"]

[[eval.cases]]
prompt = "Aura’s predictive analytics is giving inconsistent outputs. Can you explain why?"
score = { min = 8 }
criteria = ["technical_accuracy", "completeness", "clarity"]

[[eval.cases]]
prompt = "How can I integrate Aura with my existing blockchain data pipeline?"
score = { min = 8 }
criteria = ["technical_accuracy", "completeness", "actionability"]
